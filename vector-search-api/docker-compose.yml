services:
  # Vector Search API
  vector-api:
    env_file:
      - .env
    build:
      context: .
      dockerfile: Dockerfile
    image: vector-search-api:v1.0.0
    container_name: vector-search-api
    ports:
      - "5200:5200"
    environment:
      - QDRANT_URL=http://qdrant:6333
      - DEFAULT_COLLECTION=docs_2025
      - ALLOW_MODELS=st:./models/bge-m3,st:./models/mE5-small,st:./models/mE5-base,st:./models/mE5-large,st:./models/paraphrase-ml,st:./models/ko-sbert,st:./models/ko-sroberta,st:./models/ko-simcse
      - API_KEY=
      - CORS_ALLOW_ORIGINS=*
      - DEVICE=auto
      - TORCH_NUM_THREADS=4
      - API_HOST=0.0.0.0
      - API_PORT=5200
      - HF_HOME=/app/.cache/huggingface
      - TRANSFORMERS_CACHE=/app/.cache/huggingface
      - SENTENCE_TRANSFORMERS_HOME=/app/.cache/huggingface
      # Offline mode for closed network
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
    volumes:
      # Mount local models directory to container (read-only)
      - ./models:/app/models:ro
      # Persist Hugging Face cache
      - model_cache:/app/.cache/huggingface
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5200/health"]
      interval: 30s
      timeout: 10s
      retries: 5

volumes:
  model_cache:
    driver: local