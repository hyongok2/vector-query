# GPU ë°°í¬ ì „ëµ ê°€ì´ë“œ

GPU ê°€ì†ì„ ìœ„í•œ ë°°í¬ ì˜µì…˜ê³¼ ì „ëµ

## ğŸ¯ í•µì‹¬ ì „ëµ

### 1. Requirements ë¶„ê¸° ì „ëµ

**í˜„ì¬ (CPU)**: `requirements.txt`
**GPUìš©**: `requirements-gpu.txt` ìƒì„±

```bash
# requirements-gpu.txt
torch==2.8.0+cu121  # CUDA 12.1 ë²„ì „
torchvision==0.18.0+cu121
# ... ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€ë“¤ì€ ë™ì¼
```

### 2. Dockerfile ë©€í‹° ìŠ¤í…Œì´ì§€

```dockerfile
# === GPU ë²„ì „ ===
FROM nvidia/cuda:12.1-runtime-ubuntu22.04 as gpu-base
# GPU ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements-gpu.txt .
RUN pip install -r requirements-gpu.txt

# === CPU ë²„ì „ ===
FROM python:3.11-slim as cpu-base
COPY requirements.txt .
RUN pip install -r requirements.txt

# === ìµœì¢… ìŠ¤í…Œì´ì§€ ===
FROM ${BASE_IMAGE:-cpu-base} as final
# ê³µí†µ ì„¤ì •ë“¤...
```

### 3. Docker Compose í™˜ê²½ë³„ ë¶„ë¦¬

**CPU ë°°í¬**: `docker-compose.yml` (í˜„ì¬)
**GPU ë°°í¬**: `docker-compose.gpu.yml`

```yaml
# docker-compose.gpu.yml
services:
  vector-api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BASE_IMAGE: gpu-base
    runtime: nvidia  # NVIDIA Container Runtime
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - DEVICE=cuda
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

## ğŸš€ ë°°í¬ ì˜µì…˜

### Option 1: ë¹Œë“œ íƒ€ì„ ì„ íƒ
```bash
# CPU ë¹Œë“œ (ê¸°ë³¸)
docker-compose up -d

# GPU ë¹Œë“œ
docker-compose -f docker-compose.gpu.yml up -d
```

### Option 2: ëŸ°íƒ€ì„ í™˜ê²½ë³€ìˆ˜
```bash
# ë‹¨ì¼ Dockerfile, í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´
ENV DEVICE=auto  # auto|cpu|cuda|mps
```

### Option 3: ë³„ë„ ì´ë¯¸ì§€ íƒœê·¸
```bash
# CPU ì´ë¯¸ì§€
docker build -t vector-api:cpu .

# GPU ì´ë¯¸ì§€
docker build -t vector-api:gpu -f Dockerfile.gpu .
```

## ğŸ› ï¸ êµ¬í˜„ ìš°ì„ ìˆœìœ„

### 1ë‹¨ê³„: Requirements ë¶„ë¦¬
- [x] `requirements.txt` (CPU, í˜„ì¬)
- [ ] `requirements-gpu.txt` ìƒì„±

### 2ë‹¨ê³„: Docker ì„¤ì •
- [ ] `Dockerfile.gpu` ë˜ëŠ” ë©€í‹°ìŠ¤í…Œì´ì§€ ì¶”ê°€
- [ ] `docker-compose.gpu.yml` ìƒì„±

### 3ë‹¨ê³„: í™˜ê²½ ê°ì§€
```python
# app/config.py ìˆ˜ì •
def get_device():
    if device_config == "auto":
        return "cuda" if torch.cuda.is_available() else "cpu"
    return device_config
```

### 4ë‹¨ê³„: ë¬¸ì„œí™”
- [ ] README.mdì— GPU ë°°í¬ ì„¹ì…˜ ì¶”ê°€
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì¸í”„ë¼ ì¤€ë¹„
- [ ] NVIDIA Docker Runtime ì„¤ì¹˜
- [ ] CUDA 12.1+ ë“œë¼ì´ë²„ í™•ì¸
- [ ] GPU ë©”ëª¨ë¦¬ ìš©ëŸ‰ í™•ì¸ (ëª¨ë¸ë‹¹ 2-8GB)

### ì½”ë“œ ìˆ˜ì •
- [ ] `requirements-gpu.txt` ìƒì„±
- [ ] `Dockerfile.gpu` ìƒì„±
- [ ] `docker-compose.gpu.yml` ìƒì„±
- [ ] í™˜ê²½ë³€ìˆ˜ `DEVICE=cuda` ì„¤ì •

### í…ŒìŠ¤íŠ¸
- [ ] GPU ê°ì§€ í™•ì¸: `torch.cuda.is_available()`
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (CPU vs GPU)

## ğŸ ì¶”ê°€ ìµœì í™”

### ëª¨ë¸ ìµœì í™”
```python
# ë” í° ë°°ì¹˜ ì‚¬ì´ì¦ˆ í™œìš©
BATCH_SIZE = 32 if device == "cuda" else 8

# í˜¼í•© ì •ë°€ë„ (FP16)
model.half() if device == "cuda" else model.float()
```

### ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
if device == "cuda":
    torch.cuda.empty_cache()
```

## ğŸ’¡ ê¶Œì¥ì‚¬í•­

1. **ì ì§„ì  ì „í™˜**: CPU â†’ GPU í™˜ê²½ ë‹¨ê³„ì  ë§ˆì´ê·¸ë ˆì´ì…˜
2. **í™˜ê²½ ë¶„ë¦¬**: ê°œë°œ(CPU), ìš´ì˜(GPU) í™˜ê²½ êµ¬ë¶„
3. **ëª¨ë‹ˆí„°ë§**: GPU ë©”ëª¨ë¦¬/ì‚¬ìš©ë¥  ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
4. **ë¹„ìš© ìµœì í™”**: í•„ìš” ì‹œì—ë§Œ GPU ì¸ìŠ¤í„´ìŠ¤ í™œì„±í™”

**ê²°ë¡ **: í˜„ì¬ CPU ë²„ì „ì„ ìœ ì§€í•˜ë©´ì„œ GPU ì˜µì…˜ì„ ì¶”ê°€í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ ê¶Œì¥